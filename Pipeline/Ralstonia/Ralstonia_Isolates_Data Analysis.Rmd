---
title: "Data Analysis of Ralstonia Samples"
author: "Prakki Sai Rama Sridatta"
date: "`r format(Sys.Date())`"
output:
  html_document:
    keep_md: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Step 1: Raw read analysis fastqc and multiqc

The following analysis is only done 16 environmental samples. Rest 183 samples are previously done with analysis with the same steps.
The results of these 16 samples are appended to 183 samples. 

```{r rawdata, engine = 'bash', eval = FALSE}
#Location -
# cd /storage/data/DATA4/analysis/34_Ralstonia_Analysis_2022/

###################################################################

# Step 1.fastqc on Raw Reads 
##############################

mkdir 1_Rawdata

cd 1_Rawdata

# Copying softlinks of the raw data files
for d in $(cat sample.list); do ln -s $d .; done

# Moving into fastq files into respective folders
for d in $(ls *.gz | cut -f1 -d "_"  | sort -u); do mkdir $d; mv "$d"_*.gz $d; done

# Run fastqc on the raw reads (Env + 11F)
time for d in $(ls -d */| sed 's/\///g'); do echo $d; cd $d; /storage/apps/FastQC/fastqc -t 48 *.fq.gz; cd ..; done

#combine all fastqc reports using multiqc
cd 1_Rawdata

multiqc .

# Unzip the gz files - because these are softlinks we write to  an other file

time for x in $(ls -d */ | tr -d '/'); 
do 
cd $x; 
echo $x; 
  for d in $(ls *.fq.gz); 
  do 
  base=`echo $d | sed 's/.fq.gz//'`; 
  gunzip -c $d >"$base".fq & 
  done; 
cd ..;
done

# Observations from raw fastq: Adapters are present in many sequences. So, trimmed them using BBMAP.
# But base quality of all the samples looking high (above Q30).

# For generating read stats - Run this on both Env and 11F folders
$ time for d in $(ls */*fq); do echo $d; perl /storage/apps/SNP_Validation_Scripts/tools/Q20_Q30_Stats_wo_functions.pl $d & done 

# TO find the Q20_Q30 Stats faster, lets write the commands to shell script file
$ time for d in $(ls */*fq); do echo "perl /storage/apps/SNP_Validation_Scripts/tools/Q20_Q30_Stats_wo_functions.pl $d"; done >parallel_script_Q20_Q30_stats.sh
time parallel < parallel_script_Q20_Q30_stats.sh &

$ pwd
#/storage/data/DATA4/analysis/30_Aqueous_Env_11F_data_analysis/1_Rawdata

$ cat */Q30_Q20_readstats.txt >rawData_Q30_Q20_readstats.txt

# Run qualStats.R script with the rawData_Q30_Q20_readstats.txt as an input. This should generate "_Pre-Trimmed_aggregated_stats.txt" file

$ pwd
# /storage/data/DATA4/analysis/30_Aqueous_Env_11F_data_analysis/2_Cleandata/illumina_bbmap_trimmed

# Run qualStats.R script with the rawData_Q30_Q20_readstats.txt as an input. This should generate "_Pre-Trimmed_aggregated_stats.txt" file

# Run qualStats.R script with the filtData_Q30_Q20_readstats.txt as an input. This should generate "_Post-Trimmed_aggregated_stats.txt" file

# Now combine both Raw and Filt Data using qualStats_aggregated.R script


```

### Step 2: Pre-process Illumina reads 

Now let us trim the raw data from illumina using BBDuk tool

```{r trimdata_Illumina, engine = 'bash', eval = FALSE}
time for d in $(ls -d */); 
do 
  echo $d; 
  subdir=`echo $d`; 
  cd $subdir; 
  R1=`ls *_1.fq | sed 's/.fq//g'`; R2=`ls *_2.fq | sed 's/.fq//g'`; 
  echo "$R1 $R2"; 
  /storage/apps/bbmap/bbduk.sh -Xmx6g in1=$R1.fq in2=$R2.fq out1=$R1\_bbmap_adaptertrimmed.fq out2=$R2\_bbmap_adaptertrimmed.fq ref=/storage/apps/bbmap/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 qtrim=rl trimq=30 minavgquality=30; 
  cd ..; 
done

# Manually copied the bbmap command log into adapter_trimming.log

mkdir ../2_CleanData

# move the adapter trimmed files into another directory
mv */*bbmap_adaptertrimmed.fq ../2_CleanData/
cd ../2_CleanData/

# move to specific folder
for d in $(ls *.fq| awk -F_ '{print $1}' | sort -u); do echo $d; mkdir $d; mv "$d"_*.fq $d; done

# Run fastqc on the raw reads (Env + 11F)
time for d in $(ls -d */| sed 's/\///g'); do echo $d; cd $d; /storage/apps/FastQC/fastqc -t 48 *.fq; cd ..; done

#combine all fastqc reports using multiqc
cd 1_Rawdata

multiqc .

```

Plotting Raw Read and Clean Read Stats


```{r Illumina_readData_plots, fig.width=12, fig.height=8, message=FALSE}

library(tidyr)
library(dplyr)

library(ggplot2)
library(scales)

setwd("/data02/Analysis/Projects/8_Aqueos_samples/1_ReadData_Stats_Raw_vs_Filt")

stats <- read.csv("Aqueous_samples_AITBiotech_Pre_AND_Post_Trimming_Stats.csv", header = TRUE, sep = ",")
head(stats)

############--------CHANGE THESE VARIABLES---IMPORTANT!!------##########

samples_or_project_name <- "Aqueous"
seqCompany <- "AITBiotech" #Internal/AITBiotech
seqData_trimmed_or_not <- "Pre and Post-Trimmed" # use "Raw data" before trimming else use "After Trimming with Q30 score"  
genomeSize <- 5500000 


# Plotting

###----> Genome Coverage

ggplot(stats[order(pmax(stats$Genome_Coverage_Pre_Trimming, stats$Genome_Coverage_Post_Trimming)),], aes (x=factor(Isolate, levels=Isolate), Genome_Coverage)) +  
  geom_point(aes(y=Genome_Coverage_Pre_Trimming, color = "Genome_Coverage_Pre_Trimming"), size=1) + 
  geom_point(aes(y=Genome_Coverage_Post_Trimming, color = "Genome_Coverage_Post_Trimming"), size=1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5),legend.position = "top") + #Rotating and spacing axis labels
  expand_limits(x = c(0, NA), y = c(0, NA)) +
  xlab("Isolates") +
  ylab("Genome Coverage ") +
  ggtitle(paste0("Genome Coverage of ",samples_or_project_name," samples from ",seqCompany," (",seqData_trimmed_or_not,")")) #+ 

#  facet_wrap(~Species,scales = "free_x", nrow = 2) # ggtitle("Genome Coverage of Steno Samples from AITBiotech (After Trimming)")

ggsave("GenomeCoverage.png",dpi = 600, width = 8, height = 6, units = "in")

###----> Total Reads

ggplot(stats[order(pmax(stats$TotalReads_Pre_Trimming, stats$TotalReads_Post_Trimming)),], aes (x=factor(Isolate, levels=Isolate), Total_Reads)) +  
  geom_point(aes (y= TotalReads_Pre_Trimming, color = "TotalReads_Pre_Trimming"), size=1) + 
  geom_point(aes(y=TotalReads_Post_Trimming, color = "TotalReads_Post_Trimming"), size=1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5),legend.position = "top") + #Rotating and spacing axis labels
  expand_limits(x = c(0, NA), y = c(0, NA)) +
  scale_y_continuous(labels = unit_format(unit = "M", scale = 1e-6)) + ## need to load "scales" library - very readable
  xlab("Isolates") +
  ylab("Total Reads") +
  ggtitle(paste0("Total Reads of ",samples_or_project_name," samples from ",seqCompany," (",seqData_trimmed_or_not,")"))

ggsave("TotalReads.png",dpi = 600, width = 8, height = 6, units = "in")

###----> Mean Read length

ggplot(stats[order(pmax(stats$MeanReadLength_Pre_Trimming, stats$MeanReadLength_Post_Trimming)),], aes (x=factor(Isolate, levels=Isolate), MeanReadLength)) +  
  geom_point(aes (y= MeanReadLength_Pre_Trimming, color = "MeanReadLength_Pre_Trimming"), size=1) + 
  geom_point(aes(y=MeanReadLength_Post_Trimming, color = "MeanReadLength_Post_Trimming"), size=1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5),legend.position = "top") + #Rotating and spacing axis labels
  expand_limits(x = c(0, NA), y = c(0, NA)) +
  xlab("Isolates") +
  ylab("Mean Read Length ") +
  ggtitle(paste0("Mean Read Length of ",samples_or_project_name," samples from ",seqCompany," (",seqData_trimmed_or_not,")"))

ggsave("MeanReadLength.png",dpi = 600, width = 8, height = 6, units = "in")

  
###----> Total Bases

ggplot(stats[order(pmax(stats$TotalBases_Pre_Trimming, stats$TotalBases_Post_Trimming)),], aes (x=factor(Isolate, levels=Isolate), TotalBases)) +  
  geom_point(aes (y= TotalBases_Pre_Trimming, color = "TotalBases_Pre_Trimming"), size=1) + 
  geom_point(aes(y=TotalBases_Post_Trimming, color = "TotalBases_Post_Trimming"), size=1) +
  theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5),legend.position = "top") + #Rotating and spacing axis labels
  expand_limits(x = c(0, NA), y = c(0, NA)) +
  scale_y_continuous(labels = unit_format(unit = "MB", scale = 1e-6)) + ## need to load "scales" library - very readable
  xlab("Isolates") +
  ylab("Total Bases") +
  ggtitle(paste0("Total Bases of ",samples_or_project_name," samples from ",seqCompany," (",seqData_trimmed_or_not,")"))

ggsave("TotalBases.png",dpi = 600, width = 8, height = 6, units = "in")

```


### Step 4: SPADes assembly 

The resulting cleaned data from illumina is passed as input to `SPADes`. 

```{r assembly, engine = 'bash', eval = FALSE}
cd /storage/data/DATA4/analysis/34_Ralstonia_Analysis_2022/2_CleanData

time for d in $(ls -d */ | grep -v 'multiqc'); do echo $d; subdir=`echo -n $d | tr -d "/"`; cd $subdir; R1=`ls *_1_bbmap_adaptertrimmed.fq `; R2=`ls *_2_bbmap_adaptertrimmed.fq`; echo "$R1 $R2"; spades.py --pe1-1 "$R1" --pe1-2 "$R2" -o "$subdir"_spades --careful -t 48 >>log_spades; cd ..; done

mkdir ../3_SPAdes
mv */*_spades/ ../3_SPAdes/

#Copy and rename the assemblies
cd ../3_SPAdes/

for d in $(ls *_spades/contigs.fasta); do echo $d; prefix=`echo "$d" | cut -f1 -d "/"`; cp "$d" "$prefix"_contigs.fasta; done

mkdir ../4_SPAdes_Assemblies
mv *contigs.fasta ../4_SPAdes_Assemblies/
cd ../4_SPAdes_Assemblies/

Filter for contigs >1kb
------------------------
used bioawk installed in server

$ for d in $(ls *.fasta | sed 's/_contigs.fasta//g' ); do echo $d; /storage/apps/bioawk/bioawk -c fastx 'length($seq) >=1000 {print "\>"$name"\n"$seq}' "$d"_contigs.fasta >$d.gte1kb.contigs.fasta; done

mkdir gteq1kb
mv *gte1kb* gteq1kb/
mkdir full_assembly
mv *.fasta full_assembly/
```

### Step 5: Assembly Downstream - MLST

```{r mlst, engine = 'bash', eval = FALSE}

cd gteq1kb

# This is plotting the stats
statswrapper.sh *.fasta >gteq1kb_assemblies_summary_stats.txt

mlst *contigs.fasta | grep ".fasta" >>log_mlst_2.19

mkdir ../../5_MLST
mv log_mlst* ../../5_MLST/

```


### Step 6c: Assembly Downstream - Kraken2 

Kraken2 for species calling on adapter trimmed reads of illumina. 

```{r kraken2, engine = 'bash', eval = FALSE}

cd /storage/data/DATA4/analysis/34_Ralstonia_Analysis_2022/2_CleanData # Also did this on Env samples

$ time for d in $(ls -d */ | tr -d '/'); 
do 
  echo $d; 
  cd $d; 
  R1=`ls *_1_*.fq | sed 's/.fq//g'`; R2=`ls *_2_*.fq | sed 's/.fq//g'`; 
  echo "$R1 $R2"; 
  kraken2 --db /storage/apps/Kraken2/Kraken2_db/minikraken_8GB_20200312 --threads 48 --report "$d"_kraken2_report --paired "$R1".fq "$R2".fq --output "$d"_kraken2_result; 
  cd ../; 
done

--> Top species

for d in $(ls */*report); do echo $d; egrep -m1 "\sS\s" $d; done >>ALL_kraken2_report_top1species 
for d in $(ls */*report); do echo $d; egrep -m2 "\sS\s" $d; done >>ALL_kraken2_report_top2species

mkdir ../6_KRAKEN2
mv *kraken2* ../6_KRAKEN2
mv */*kraken2* ../6_KRAKEN2

```


### Step 7: Assembly Downstream - AMRFinderplus

Although Resfinder is part of CGE-BAPS, only version 2.1 is used for finding the resistance genes. So, we downloaded the resfinder4.1 version locally on the server which is fast to detect resistance genes.

Running resfinder on all assembly fasta - default setting is coverage length of the sequence is 60% and identity 80%

```{r amrfinderplus, engine = 'bash', eval = FALSE}

time for d in $(ls *.fasta); do /storage/apps/amrfinder/amrfinder --plus -n $d > "$d"_amrfinder_out --threads 48; done &
awk '{print FILENAME "\t" $0}' *_amrfinder_out >Combined_Ralstonia_insidiosa_AMRFinderPlus_out

# Single line resistome format

#awk '{print FILENAME "\t" $0}' */ResFinder_results_tab.txt  | tr ' ' '_' | cut -f1-9 -d "       " | sed -e 's/_spades.gte1kb.contigs.fasta_resfinder_outdir\/ResFinder_results_tab.txt//g' | awk '$3>=80 && $5>=80'| awk '{print $1 "\t" $2}' | sort -u | grep -v 'Resistance_gene' | groupBy -g 1 -c 2 -o collapse  >cge_resfinder_resistome.tab

# head(cge_resfinder_resistome.tab) 

#A1089	blaACT-16,blaOXA-181,fosA,mdf(A),OqxA,OqxB,qnrS1
#A1416	aac(6')-Ib-cr,aadA16,ARR-3,blaCTX-M-15,blaSHV-11,...

mkdir ../../7_CGE
mv *_out* ../../7_CGE/
```


### Step 7a: Plot resistome

Here ResFinder_results_tab.txt results are taken from each directory | remove spaces | unwanted columns removed | trimmed unnecessary texts | filtered the AMR genes which has coverage and identity >=80% | extracted unique records | collapsed the genes for each of the sample


```{r plot_resistome, fig.width=12, fig.height=10, message=FALSE}


```



### Step 8: Core genome analysis - Prokka/Roary


```{r coregenome, engine = 'bash', eval = FALSE}
cd /storage/data/DATA4/analysis/34_Ralstonia_Analysis_2022/4_SPAdes_Assemblies/gteq1kb

# Since we are not performing coregenome by ST, we do not need to segregate fasta files by ST

# Run Prokka
# A duplicate local version Prokka called "Prokka2" is created in same folder to circumvent maxcontig length ID error
# by increasing from 37 (prokka script) to 50 (prokka2 script)

$ time for d in $(ls *.fasta | sed 's/_spades.gte1kb.contigs.fasta//g'); 
do 
echo "$d"; 
prokka2 --force --cpus 32 --outdir "$d"_prokka_out --prefix "$d" "$d"_spades.gte1kb.contigs.fasta >>prokka_log 2>>prokka_error; 
done

mkdir ../../8_Prokka
mv *prokka* ../../8_Prokka
# Added previously run prokka output folders for 179 PAE samples into this folder with 16 environmental samples

mkdir ../../9_Roary
cd ../../8_Prokka
cp */*.gff ../9_Roary/

#9. Roary
########

cd ../9_Roary/
time roary -e --mafft -g 60000 -p 48 -cd 95 *.gff

real	1m47.457s
user	32m4.039s
sys	3m54.852s

# --> Create a list of all the samples according to Species (uusually it is by ST type) to be SNP validated
# If according to STs, then this oneliner generates list of files with the sample IDs according to the ST assigned by MLST tool
# Make sure there is a tab delimted space when using the fgrep

#for d in $(cat ../5_MLST/log_mlst | awk '{print $3}' | sort -u);do
#num=`grep -P "\t$d\t" ../5_MLST/log_mlst | wc -l`;
#grep -P "\t$d\t" ../5_MLST/log_mlst | awk '{print $1 "\t" $3}' | sed 's/_spades.*//g' >list_PAE_ST"$d"_"$num"samples;
#done

#$ cp list_PAE_ST*samples ../11_SNP_Validation/
mkdir ../10_SNP_Validation/
cp core_gene_alignment.aln  ../10_SNP_Validation/
ls *.gff | sed 's/.gff//g' >../10_SNP_Validation/list_RI_7samples

#10. SNP Validation
###################

cd ../10_SNP_Validation/

cp /storage/apps/SNP_Validation_Scripts/tools/snpvalidation.sh .

In the snpvalidation.sh script, change the path of the directory for R1 and R2 to run snippy. 

--R1 /storage/data/DATA4/analysis/27_Paeruginosa_183_samples_Shawn_with_16_Envsamples_31_Jeanette_40_jocelynsamples/2_CleanData/$d/"$d"*_1_bbmap_adaptertrimmed.fastq 
--R2 /storage/data/DATA4/analysis/27_Paeruginosa_183_samples_Shawn_with_16_Envsamples_31_Jeanette_40_jocelynsamples/2_CleanData/$d/"$d"*_2_bbmap_adaptertrimmed.fastq

$ time ./snpvalidation.sh list_RI_7samples

real	13m32.523s
user	106m54.776s
sys	7m36.499s



11. Gubbins (We no more do padding before gubbins) &&  Calculate SNP difference
#################################################################################

# Once the snpvalidation is done, change directory into the PAE_ST308_179samples, and run gubbins

cd /storage/data/DATA4/analysis/34_Ralstonia_Analysis_2022/10_SNP_Validation/RI_7samples

mkdir Gubbins
cp core_gene_alignment_withConsensus_convertXtoN_convertgaptoLittleN_editedByGATK_renamed.fasta Gubbins/
cd Gubbins/

# Convert the consensus alignment into one liner fasta format

$ perl /storage/apps/SNP_Validation_Scripts/tools/convertFastatoOneLine.pl core_gene_alignment_withConsensus_convertXtoN_convertgaptoLittleN_editedByGATK_renamed.fasta

# Assuming consensus is on top remove consensus sequence from the above fasta file
sed '1,2d' core_gene_alignment_withConsensus_convertXtoN_convertgaptoLittleN_editedByGATK_renamed.fasta_OneLine.fasta >>core_gene_alignment_woConsensus_editedByGATK_renamed.fasta

# Gubbins - Filter recombination
$ time run_gubbins.py --prefix postGubbins --filter_percentage 100 --threads 48 core_gene_alignment_woConsensus_editedByGATK_renamed.fasta --verbose 

$ cp /storage/apps/SNP_Validation_Scripts/tools/dnaDist.r .

$ Rscript dnaDist.r 



#(Repeat the above steps in Gubbins for all the Species (or ST groups))

# Since it is too tiring to run each and every step, I created a shell script which run the above commands for the rest of the species. So we can run all the steps indivisually or run the below script if there are multiple Species and ST to deal with 

# $ time bash runProkka_Roary_Gubbins_bySpecies.sh


12. Clustering 
###############

# We want to cluster all the isolates which have a SNP difference  with certain threshold (for this case : 0 SNPs, Therefore my awk has $NF==0)

# The following one-liner takes the SNP difference file | takes pair1,pair2,SNP Diff | removes double quotes if any | extract pairs satisfyingSNP Diff threshold (Here 0 SNPs) | print rhe pair1, pair2 to a file

$ cat postGubbins.filtered_polymorphic_sites_melt_sorted.csv | awk -F',' '{print $2"\t"$3"\t"$4}' |  tr -d "\"" | awk '$NF<=2' | awk '{print $1"\t"$2}' >pairsFile.list

# The Clustering Script will recruit the isolates if any of the isoaltes has link with another to the group

$ cp /storage/apps/PlasmidSeeker/Databases_aftr_04122019/ECCMID_2020/Deduplicated_Fasta_for_PlasmidSeeker_DB/ClustrPairs_v3.pl .

$ perl ClustrPairs_v3.pl pairsFile.list 

# Looking at thresholds

#for d in {0..11}; do cat postGubbins.filtered_polymorphic_sites_melt_sorted.csv | awk -F',' '{print $2"\t"$3"\t"$4}' |  tr -d "\"" | awk -v cutoff="$d" '$NF<=cutoff' | awk '{print $1"\t"$2}' >pairsFile__snpscutoff__"$d".list; clusters=`perl /storage/apps/SNP_Validation_Scripts/tools/ClustrPairs_v3.pl pairsFile__snpscutoff__"$d".list | fgrep -c "Cluster:"`; perl /storage/apps/SNP_Validation_Scripts/tools/ClustrPairs_v3.pl pairsFile__snpscutoff__"$d".list >Clusters__snpscutoff__"$d".list; samples=`grep -v 'Cluster:' Clusters__snpscutoff__"$d".list | wc -l`; echo "At SNP cutoff $d - Clusters: $clusters - with $samples samples";  done 

```

### Step 9: Iq-tree for generating ML tree

We have two species Ralstonia_mannitolilytica and insidiosa

```{r beast, engine = 'bash', eval = FALSE}

# Ralstonia_mannitolilytica
cd /storage/data/DATA4/analysis/34_Ralstonia_mannitolilytica_2021/8_SNP_Validation_added_ADH0093/RM_4samples/Gubbins

# iqtree first which generates a tree

time iqtree2 -s postGubbins.filtered_polymorphic_sites_BEAST_withDates.fasta -T AUTO

```

### Step 10: ggtree + SNP matrix
